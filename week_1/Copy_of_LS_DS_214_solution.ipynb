{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LS_DS_214_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvkRNq4wAxw1"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2DyuwuPAxw2"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'\n",
        "\n",
        "# Import Model Building libaries\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Import libaries for encoding data\n",
        "from category_encoders import OneHotEncoder\n",
        "\n",
        "# Import Visualization libaries\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O_6NCqlAxw3"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNpwB9GtAxw4"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcaY0rD2Axw4"
      },
      "source": [
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date')\n",
        "    \n",
        "    # Drop unrated burritos\n",
        "    df.dropna(subset=['overall'], inplace=True)\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
        "\n",
        "    # Function to replace space with nospace\n",
        "    def nospace(x):\n",
        "      return x.replace(\" \", \"\")\n",
        "    df = df.apply(nospace)\n",
        "\n",
        "    # function to replace all x with 0\n",
        "    def newx(x):\n",
        "      return x.replace(\"x\", \"0\")\n",
        "    df = df.apply(newx)\n",
        "\n",
        "    # function to replace all X with 1 \n",
        "    def new_x(x):\n",
        "      return x.replace(\"X\", \"1\")\n",
        "    df = df.apply(new_x)\n",
        "\n",
        "    # function to replace all No with 0\n",
        "    def new_n(x):\n",
        "      return x.replace(\"No\",\"0\")\n",
        "    df = df.apply(new_n)\n",
        "\n",
        "    # function to replace all Yes with 1\n",
        "    def new_y(x):\n",
        "      return x.replace(\"Yes\",\"1\")\n",
        "    df = df.apply(new_y)\n",
        "\n",
        "    # Columns for int conversion\n",
        "    cols_to_change = ['Chips','NonSD','Beef','Pico','Guac','Cheese','Fries','Sour cream','Pork','Shrimp','Rice','Beans','Tomato','Bell peper','Cabbage','Sauce','Salsa.1','Cilantro','Onion']\n",
        "\n",
        "    # function to replace all strings to int values\n",
        "    df[cols_to_change] = df[cols_to_change].applymap(lambda x: 1 if type(x) == str else 0)\n",
        "\n",
        "    # function to take all of the following's column and replace them with binary values, then convert it to an int\n",
        "    burrito_type = ['california','asada','surf','carnitas']\n",
        "    for b in burrito_type:\n",
        "      df[b] = df['Burrito'].str.lower().str.contains(b).astype(int)\n",
        "\n",
        "    # Drop columns that are going to mess with the model in some way\n",
        "    df = df.drop(columns=['Notes','Location','Address','URL','Neighborhood','Unreliable','Queso','Sushi','Chile relleno','Zucchini','Lobster','Bacon','Fish','Mushroom','Ham','Avocado','Chicken','Nopales','Avocado','Corn','Taquito','Egg','Pineapple','Carrots','Lettuce','Rec', 'overall','Burrito','Reviewer'])\n",
        "\n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "\n",
        "# Using the wrangle function created above to clean the dataset before training models\n",
        "df = wrangle(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWKnyUP7Axw4"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxsNWSPpIMLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f3157b-7ec8-49e6-d91b-2ec161d3221e"
      },
      "source": [
        "# EDA\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Yelp', 'Google', 'Chips', 'Cost', 'Hunger', 'Mass (g)',\n",
              "       'Density (g/mL)', 'Length', 'Circum', 'Volume', 'Tortilla', 'Temp',\n",
              "       'Meat', 'Fillings', 'Meat:filling', 'Uniformity', 'Salsa', 'Synergy',\n",
              "       'Wrap', 'NonSD', 'Beef', 'Pico', 'Guac', 'Cheese', 'Fries',\n",
              "       'Sour cream', 'Pork', 'Shrimp', 'Rice', 'Beans', 'Tomato', 'Bell peper',\n",
              "       'Cabbage', 'Sauce', 'Salsa.1', 'Cilantro', 'Onion', 'Great',\n",
              "       'california', 'asada', 'surf', 'carnitas'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJRUlksJlodP"
      },
      "source": [
        "# EDA\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyaNdnIuaMpp"
      },
      "source": [
        "  threshold = .75*len('Year')\n",
        "  cols_to_drop = []\n",
        "  for cols in df.select_dtypes('object'):\n",
        "    if df[cols].nunique() > threshold:\n",
        "      cols_to_drop.append(cols)\n",
        "print('High Card:',cols_to_drop)\n",
        "print('---------------------------------------------------')\n",
        "print(df['Length'].value_counts())\n",
        "print('---------------------------------------------------')\n",
        "print(df['Google'].value_counts())\n",
        "print('---------------------------------------------------')\n",
        "print(df['Great'].value_counts())\n",
        "print('---------------------------------------------------')\n",
        "print(df['Tomato'].value_counts())\n",
        "print('---------------------------------------------------')\n",
        "print('Percentage of null values within dataset')\n",
        "print('===================================================')\n",
        "((df.isnull()).sum()*100/(len(df))).round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6enekKdlxB7"
      },
      "source": [
        "# EDA\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EidRCLg9Axw5"
      },
      "source": [
        "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
        "\n",
        "```\n",
        "0      x\n",
        "1      x\n",
        "2    NaN\n",
        "3      x\n",
        "4      x\n",
        "Name: Beef, dtype: object\n",
        "```\n",
        "\n",
        "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox8FOuyFAxw5"
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ardYnGWLAxw6"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWgpYBGRAxw6"
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbChoRm6Axw6"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h9MD9guAxw7"
      },
      "source": [
        "# Split your dataset into the feature matrix X and the target vector y. You want to predict 'Great'\n",
        "target='Great'\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUBtXYxuAxw7"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFRIi1G2Axw7"
      },
      "source": [
        "# Split X and y into a training set (X_train, y_train) and a test set (X_test, y_test)\n",
        "mask = X.index.year < 2018\n",
        "X_train, y_train = X.loc[mask], y.loc[mask]\n",
        "X_test, y_test = X.loc[~mask], y.loc[~mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_-eQXcAxw7"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmt6wpbgAxw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94496495-dc0e-43f0-f069-0697217dd0f1"
      },
      "source": [
        "# Figure out what is the majority class in y_train and what percentage of your training observations it represents\n",
        "baseline_acc = y_train.value_counts(normalize=True)\n",
        "print('Baseline Accuracy Score:', baseline_acc.max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Accuracy Score: 0.5822454308093995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyPR3W2Axw8"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUNKukb5Axw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888b3827-d12c-434b-93cc-86a8f8366fe7"
      },
      "source": [
        "# Build a Pipeline named model_logr, and fit it to your training data\n",
        "model_logr = make_pipeline(\n",
        "    OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()\n",
        ")\n",
        "model_logr.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
            "  elif pd.api.types.is_categorical(cols):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('onehotencoder',\n",
              "                 OneHotEncoder(cols=[], drop_invariant=False,\n",
              "                               handle_missing='value', handle_unknown='value',\n",
              "                               return_df=True, use_cat_names=True, verbose=0)),\n",
              "                ('simpleimputer',\n",
              "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
              "                               missing_values=nan, strategy='mean',\n",
              "                               verbose=0)),\n",
              "                ('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTRNBfh-Axw8"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUCk2BKTAxw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffcb2d3-02db-4a3b-9a14-f352b2efdb85"
      },
      "source": [
        "# Calculate the training and test accuracy score for model_logr\n",
        "training_acc = model_logr.score(X_train,y_train)\n",
        "test_acc = model_logr.score(X_test,y_test)\n",
        "\n",
        "# Print the results:\n",
        "print('Training Accuracy Score:', training_acc)\n",
        "print('===================================================')\n",
        "print('Test Accuracy Score:', test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy Score: 0.8903394255874674\n",
            "===================================================\n",
            "Test Accuracy Score: 0.7368421052631579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWhyzztAxw9"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csteYmcgAxw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "ebbab523-d6c9-44c9-e283-32e8879d2269"
      },
      "source": [
        "# Create your horizontal barchart here.\n",
        "coefficients = model_logr.named_steps['logisticregression'].coef_[0]\n",
        "features = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
        "feat_imp = pd.Series(coefficients, index=features).sort_values()\n",
        "feat_imp.tail(10).plot(kind='barh')\n",
        "plt.title('Coefficients for Logistic Regression');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0NcGgVbAxw9"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKL0MlztAxw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82279d1d-dc02-478c-b8c9-1902619cc630"
      },
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
        "\n",
        "y_pred = model_logr.predict(X_test)\n",
        "y_preda = model_logr.predict_proba(X_test)\n",
        "\n",
        "print(y_pred)\n",
        "print('----------------------------------------------------------')\n",
        "print(y_preda)\n",
        "print('==========================================================')\n",
        "print(y_pred.shape)\n",
        "print('----------------------------------------------------------')\n",
        "print(y_preda.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1]\n",
            "----------------------------------------------------------\n",
            "[[4.66170269e-05 9.99953383e-01]\n",
            " [1.47413290e-03 9.98525867e-01]\n",
            " [9.99672693e-01 3.27306804e-04]\n",
            " [1.95945292e-05 9.99980405e-01]\n",
            " [9.99146929e-01 8.53070856e-04]\n",
            " [8.39026291e-01 1.60973709e-01]\n",
            " [2.00687744e-02 9.79931226e-01]\n",
            " [2.23523264e-04 9.99776477e-01]\n",
            " [1.44017844e-01 8.55982156e-01]\n",
            " [7.12300930e-02 9.28769907e-01]\n",
            " [3.27568306e-01 6.72431694e-01]\n",
            " [9.52006063e-01 4.79939372e-02]\n",
            " [1.35353013e-01 8.64646987e-01]\n",
            " [2.15113285e-01 7.84886715e-01]\n",
            " [1.45011607e-01 8.54988393e-01]\n",
            " [4.00914547e-02 9.59908545e-01]\n",
            " [5.81450151e-02 9.41854985e-01]\n",
            " [9.94516250e-01 5.48375027e-03]\n",
            " [9.98540407e-01 1.45959300e-03]\n",
            " [9.94701424e-01 5.29857620e-03]\n",
            " [9.90706606e-01 9.29339411e-03]\n",
            " [2.04518684e-02 9.79548132e-01]\n",
            " [8.80847705e-01 1.19152295e-01]\n",
            " [6.92317967e-01 3.07682033e-01]\n",
            " [2.41372758e-01 7.58627242e-01]\n",
            " [9.51321857e-01 4.86781429e-02]\n",
            " [9.46116237e-01 5.38837629e-02]\n",
            " [1.18349824e-05 9.99988165e-01]\n",
            " [3.88765521e-02 9.61123448e-01]\n",
            " [4.98816123e-03 9.95011839e-01]\n",
            " [9.08100465e-01 9.18995352e-02]\n",
            " [2.13830338e-01 7.86169662e-01]\n",
            " [4.07252721e-02 9.59274728e-01]\n",
            " [9.87165961e-01 1.28340388e-02]\n",
            " [2.34641404e-01 7.65358596e-01]\n",
            " [2.79594210e-01 7.20405790e-01]\n",
            " [2.09116587e-04 9.99790883e-01]\n",
            " [1.97590935e-02 9.80240907e-01]]\n",
            "==========================================================\n",
            "(38,)\n",
            "----------------------------------------------------------\n",
            "(38, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC53XsTnAxw9"
      },
      "source": [
        "**Give your written answer here:**\n",
        "\n",
        "```\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "* The output is a numpy array\n",
        "\n",
        "- What are the shapes of their different output?\n",
        "* y_pred - 38,0\n",
        "* y_preda - 38,2\n",
        "\n",
        "- What numerical values are in the output?\n",
        "*Binary output that shows if a burrito is great or not\n",
        "\n",
        "- What do those numerical values represent?\n",
        "*The proability % of the output\n",
        "```"
      ]
    }
  ]
}